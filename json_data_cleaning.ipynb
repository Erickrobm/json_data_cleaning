{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='Text-align: center;'>**Cleaning & Converting Data from a JSON file into a CSV**</h1>\n",
    "\n",
    "`Created by: Erick Eduardo Robledo Montes`\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "<p style='Text-align: justify;'><i>Description:</i> Cleaning and converting data from the JSON file into a CSV format that is more user-friendly. The script performs various data cleaning tasks such as removing irrelevant columns, handling missing values, and converting data types to make the data more consistent and usable. The cleaned data is then exported to a CSV file, which can be easily imported into a spreadsheet or other data analysis tool. </p>\n",
    "\n",
    "*Link: [https://secure.toronto.ca/nm/notices.json]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "---\n",
    "* The `DataScraper` class is responsible for retrieving and loading the JSON data from the specified URL. This class would likely contain methods for making a web request to the URL, parsing the JSON data, and loading it into a Python object.\n",
    "\n",
    "* The `DataCleaner` class is responsible for cleaning and preprocessing the data before it is exported to CSV. This class would likely contain methods for removing irrelevant columns, handling missing values, and converting data types to make the data more consistent and usable.\n",
    "\n",
    "* The `DataProcessor` class is responsible for coordinating the work of the DataScraper and DataCleaner classes, and for outputting the cleaned data to a CSV file. This class would likely contain methods for calling the appropriate methods from the DataScraper and DataCleaner classes, and for writing the cleaned data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScraper:\n",
    "    \"\"\"\n",
    "    This class is responsible for scraping data from a website and\n",
    "    converting it into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, url: str):\n",
    "        \"\"\"\n",
    "        Initialize the DataScraper object with the specified URL.\n",
    "\n",
    "        :param url: The URL of the website to scrape data from.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.data = None\n",
    "        self.df = None\n",
    "\n",
    "    def scrape_data(self):\n",
    "        \"\"\"\n",
    "        Make a GET request to the website and store the JSON data in the\n",
    "        'data' attribute.\n",
    "        \"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        if response.status_code == 200:\n",
    "            self.data = response.json()\n",
    "            print(f\"Success. Status code:\", response.status_code)\n",
    "        else:\n",
    "            print(f\"Error. Status code:\", response.status_code)\n",
    "\n",
    "    def convert_data_to_df(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Convert the JSON data stored in the 'data' attribute into a pandas\n",
    "        DataFrame and store it in the 'df' attribute.\n",
    "        \"\"\"\n",
    "        for key in self.data.keys():\n",
    "            self.data[key] = pd.Series(self.data[key])\n",
    "        # The column \"Records\" stored multiple columns to work with\n",
    "        data_list = [self.data[column_name][i] for i in range(len(self.data[column_name]))]\n",
    "        self.df = pd.concat([pd.DataFrame.from_dict(d, orient='index').T for d in data_list], ignore_index=True)\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"\n",
    "    This class is responsible for cleaning the data in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the DataCleaner object with the specified DataFrame.\n",
    "\n",
    "        :param df: The DataFrame to clean.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Perform data cleaning operations on the DataFrame.\n",
    "        \"\"\"\n",
    "        # Remove non-integer values from the column 'noticeId'\n",
    "        self.df = self.df[self.df['noticeId'].apply(lambda x: isinstance(x, int))]\n",
    "        # Sort the 'noticeId' column in ascending order\n",
    "        self.df.sort_values(by='noticeId', ascending=True, inplace=True)\n",
    "        # Drop column who only has empty lists\n",
    "        self.df.drop(columns='planningApplicationNumbers', inplace=True)\n",
    "        # Transform the column 'noticeDate' to datetime\n",
    "        self.df['noticeDate'] = pd.to_datetime(self.df['noticeDate'], unit='ms')\n",
    "        # Remove HTML tags using regular expressions\n",
    "        self.df['noticeDescription'] = self.df['noticeDescription'].apply(lambda x: re.sub('<[^<]+?>', '', x))\n",
    "        # Remove NaN values\n",
    "        self.df[\"accessibilityDescription\"].replace(np.nan, '', inplace=True)\n",
    "        self.df[\"decisionBody\"].replace(np.nan, '', inplace=True)\n",
    "        # Call the 'clean_column' function to keep going on data cleaning\n",
    "        self.clean_column(\"contact\")\n",
    "        self.clean_column(\"eventList\")\n",
    "        self.clean_column(\"backgroundInformationList\") \n",
    "        self.clean_column(\"addressList\")\n",
    "        self.clean_column(\"otherReferenceList\")\n",
    "        self.remove_data_from_column(\"uniqueMapUrl\")\n",
    "        self.remove_data_from_column(\"topics\")\n",
    "    \n",
    "    def clean_column(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Perform common cleaning operations on a specified column.\n",
    "        :param column_name: The name of the column to clean.\n",
    "        \"\"\"\n",
    "        column_data = []\n",
    "        if column_name == \"contact\":\n",
    "            column_list = list(self.df[column_name])\n",
    "            for i, val in enumerate(column_list):\n",
    "                if pd.isnull(val):\n",
    "                    column_list[i] = {}\n",
    "            column_data = [x for x in column_list if type(x)==dict]\n",
    "        else:\n",
    "            column_list = [self.df[column_name][i] for i in range(len(self.df))] \n",
    "            for i in range(len(self.df)):\n",
    "                if column_list[i] == []:\n",
    "                    column_data.append({})\n",
    "                else:\n",
    "                    column_data.append(column_list[i][0])\n",
    "\n",
    "        # Create the DataFrame\n",
    "        column_df = pd.DataFrame(column_data)\n",
    "        # Convert all elements to strings before joining\n",
    "        column_df = column_df.astype(str)\n",
    "        # Join the elements of each row in the DataFrame\n",
    "        self.df[column_name] = column_df.apply(lambda x: \", \".join(x), axis=1)\n",
    "        while True:\n",
    "            self.df[column_name] = self.df[column_name].str.replace('^nan, |, nan, | nan |nan|, nan', '', regex=True)\n",
    "            if ', nan' not in self.df[column_name].to_string():\n",
    "                break\n",
    "    \n",
    "    def remove_data_from_column(self, column_name: str):\n",
    "        for index, row in self.df.iterrows():\n",
    "            if row[column_name]:\n",
    "                self.df.at[index, column_name] = row[column_name][0]\n",
    "            else:\n",
    "                self.df.at[index, column_name] = ''\n",
    "\n",
    "    def optimize_data(self):\n",
    "        \"\"\"\n",
    "        Perform optimization on the DataFrame.\n",
    "        \"\"\"\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    This class is responsible for processing the cleaned data.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the DataProcessor object with the specified DataFrame.\n",
    "        :param df: The cleaned DataFrame to process.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        \n",
    "    def export_data(self, file_name: str):\n",
    "        \"\"\"\n",
    "        Export the processed DataFrame to a CSV file.\n",
    "        :param file_name: The name of the CSV file to export the data to.\n",
    "        \"\"\"\n",
    "        self.df.to_csv(file_name, index=None, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing JSON Data into a CSV File Using Python Classes\n",
    "---\n",
    "* The first three lines of the code set the URL of the website to scrape, the path and file name for the exported CSV file, and the name of the column in the JSON data that contains the relevant records.\n",
    "\n",
    "* The script then creates an instance of the DataScraper class and uses its `scrape_data()` method to retrieve the data from the website. The `convert_data_to_df()` method is used to convert the JSON data into a DataFrame.\n",
    "\n",
    "* Next, an instance of the DataCleaner class is created and its `clean_data()` method is used to clean the data in the DataFrame. The `optimize_data()` method is used to optimize the data.\n",
    "\n",
    "* Finally, an instance of the DataProcessor class is created and its `export_data()` method is used to export the processed data to a CSV file. The path and file name for the exported CSV file are passed as an argument.\n",
    "\n",
    "* At the end, the script prints \"Data processing completed.\" to indicate that the data scraping, cleaning, and conversion process has been completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. Status code: 200\n",
      "Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://secure.toronto.ca/nm/notices.json\"\n",
    "path = \"dataset/\"\n",
    "file_name = \"processed_data.csv\"\n",
    "column_name = \"Records\"\n",
    "\n",
    "# Initialize the DataScraper object\n",
    "scraper = DataScraper(url)\n",
    "# Scrape data from the website\n",
    "scraper.scrape_data()\n",
    "# Convert the JSON data into a DataFrame\n",
    "scraper.convert_data_to_df(column_name)\n",
    "\n",
    "# Initialize the DataCleaner object\n",
    "cleaner = DataCleaner(scraper.df)\n",
    "# Clean the data in the DataFrame\n",
    "cleaner.clean_data()\n",
    "cleaner.optimize_data()\n",
    "\n",
    "# Initialize the DataProcessor object\n",
    "processor = DataProcessor(cleaner.df)\n",
    "# Export the processed data to a CSV file\n",
    "processor.export_data(path+file_name)\n",
    "\n",
    "print(\"Data processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UASLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d296a6862911035b23eed23915f5ea7a3ddd49d93c9440154a78263fbdd13c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
